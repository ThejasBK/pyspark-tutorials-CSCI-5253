{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Tutorial\n",
    "<div>\n",
    " <h2> CSCI 4283 / 5253 \n",
    "  <IMG SRC=\"https://www.colorado.edu/cs/profiles/express/themes/cuspirit/logo.png\" WIDTH=50 ALIGN=\"right\"/> </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark was originally developed using Scala, although there are Python and Java interfaces as well. This tutorial covers [most of the RDD API](https://spark.apache.org/docs/latest/rdd-programming-guide.html#resilient-distributed-datasets-rdds) using Python bindings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "import numpy as np\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `SparkContext` object tells Spark how to access a cluster. The `SparkConf` object defines information about our job.\n",
    "\n",
    "The `master` of a Spark configuration is the cluster (YARN or Mesos) manager. It can also be \"local\" meaning that the Spark job runs on your local machine, which is what we'll do here; the `[*]` notation means to use all the available cores. In general, you shouldn't hardcode the `master` mechanism.\n",
    "\n",
    "Spark uses a function chaining notation. We'll use that throughout unless it makes this confusing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf=SparkConf().setAppName(\"pyspark tutorial\").setMaster(\"local[*]\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Datasets\n",
    "The basic Spark data structure is the RDD (resilient distributed data), which is essentially a vector distributed across the cluster of nodes or on the local system. In PySpark, the vector can contain a heterogenous collection of types (strings, ints, etc).\n",
    "\n",
    "You can create an RDD from a list or tuple, read it from a local file or read it from networked distributions such as HDFS or S3.\n",
    "\n",
    "The following shows creating three datasets from lists using the _parallelize_ method. RDD's are broken into multiple slices which are the unit of work allocation (*i.e.*, more slices gives more potential for parallelism, but too many slices gives too much overhead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "a = sc.parallelize([7, 2, 3, 1, 2, 3, 4, 5, 6, 7], numSlices=2)\n",
    "b = sc.parallelize([2, 3, 99, 22, -77])\n",
    "c = sc.parallelize([ (1,2), (2,3), (1, 99), (3, 44), (2, 1), (4,5), (3, 19) ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "passwd = sc.textFile(\"/etc/passwd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to read and write binary data files, including data formatted in Hadoop Sequence types.\n",
    "\n",
    "Spark also supports _accumulators_ and _broadcast variables_.  Accumulators are designed to sum or aggregate values from across the cluster; they are really only suitable for commutative-associative operators. Broadcast variables are efficiently disseminated to all nodes in the cluster; they can be used for the equivilent of \"map-side joins\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations and Actions\n",
    "*Transformations* produce new RDD's by transforming existing RDD's  and *Actions* convert data *to* and *from* an RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions\n",
    "\n",
    "Some of the most simple actions are:\n",
    "* count() - Return the number of items in the RDD\n",
    "* take(_n_) - Extract and return the first _n_ items from the RDD\n",
    "* first() - Same as take(1)\n",
    "* collect() - Same as take(count()) - **returns full RDD**\n",
    "* takeSample(_withReplacement_:Boolean, _num_:int, [ seed:Int] ) - extract a random set of _num_ items from the RDD with or without replacement.\n",
    "* takeOrdered( _num_ ) - extract _num_ items from the sorted RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "[7, 2]\n",
      "7\n",
      "[7, 2, 3, 1, 2, 3, 4, 5, 6, 7]\n",
      "[3, 3, 4]\n",
      "[1, 2, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "print(a.count())\n",
    "print(a.take(2))\n",
    "print(a.first())\n",
    "print(a.collect())\n",
    "print(a.takeSample(True, 3))\n",
    "print(a.takeOrdered(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lambda Functions in Python\n",
    "\n",
    "Lambda functions, or anonymous functions are common in other languages (e.g. Scala) and commonly used in PySpark. The Python lambda is restricted to simple single-line statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(lambda x: x + x)(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add2 = lambda x: x + 2\n",
    "add2(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map, Reduce & flatMap\n",
    "\n",
    "`map` is a transformation that produces a new RDD. `reduce` is an action that applies a specified function to the elements of an RDD. Map is applied using a single-argument (unary) function (often a *lambda*) while reduce takes a binary (or dyadic) function.\n",
    "\n",
    "Examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[49, 4, 9, 1, 4, 9, 16, 25, 36, 49]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.map( lambda x: x**2 ).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 99, 44, 1, 5, 19]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.map( lambda x: x[1] ).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following `lambda` sums the first and second element of the tuples in `c`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (2, 3), (1, 99), (3, 44), (2, 1), (4, 5), (3, 19)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 173)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.reduce( lambda x,y: (x[0] + y[0], x[1] + y[1]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That should produce the sample result as the more complex example\n",
    "below, which returns an RDD for each field of the tuple and then\n",
    "adds those those using reduce. The operator.add function is \"+\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 173)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "( c.map( lambda x : x[0] ).reduce(operator.add), \n",
    "  c.map( lambda x: x[1] ).reduce(operator.add) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`flatMap` applies a map operation across elements of a list, but then takes those elements and *.appends* them to the list. The result is useful when processing a set of tuples or breaking documents into words and then processing the words rather than lines-of-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sent = sc.parallelize([\"these are some\", \"sample words\" ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['these', 'are', 'some'], ['sample', 'words']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent.map( lambda x : x.split() ).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['these', 'are', 'some', 'sample', 'words']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent.flatMap( lambda x : x.split() ).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fold` takes a \"zero value\" and a function and then repeatedly performs a reduction on the RDD using the zero value and function and then again when the values have be collected at the host. `fold` is a general version of `reduce` that handles the case of singleton data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 2, 3, 1, 2, 3, 4, 5, 6, 7]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showAdd(x,y):\n",
    "    return \"({} + {})\".format(str(x),str(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'((((1 + 2) + 3) + 4) + 5)'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oneslice = sc.parallelize([1,2,3,4,5],1)\n",
    "oneslice.reduce(showAdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(1 + (((((1 + 1) + 2) + 3) + 4) + 5))'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oneslice.fold(1,showAdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like `reduce`, the `fold` operation really only works well for commutative-associative operators because it's applied to each slice of an RDD independently.\n",
    "\n",
    "Recall that we explicitly specified that `a`should have two slices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(((((7 + 2) + 3) + 1) + 2) + ((((3 + 4) + 5) + 6) + 7))'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.reduce(showAdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'((1 + (((((1 + 7) + 2) + 3) + 1) + 2)) + (((((1 + 3) + 4) + 5) + 6) + 7))'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.fold(1, showAdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`aggregate` performs an operation like`fold` on each RDD partition and then uses a __combine function__ to join partitions.\n",
    "\n",
    "For example, assume we have data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "twoPart = sc.parallelize([1,2,3,4], numSlices=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data will (likely) be divided into `[1,2]` and `[3,4]`. Now assume we want to reduce two values -- the first is the sum of the data (10) and the second is the length of largest partition (likely 2).\n",
    "\n",
    "We'll have two distinct functions -- `seqOp` will define operations within a partition and `combOp` will define op how partitions are combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seqOp( x, y):\n",
    "    xSum, xLth = x\n",
    "    return (xSum+y, xLth + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combOp( x, y ):\n",
    "    xSum, xLth= x\n",
    "    ySum, yLth= y\n",
    "    return (xSum + ySum, max(xLth, yLth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with `fold`, we need a \"zero-value\" to start folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twoPart.aggregate( (0,0), seqOp, combOp )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following diagram (lifted from this nice [StackOverflow article](https://stackoverflow.com/questions/28240706/explain-the-aggregate-functionality-in-spark)) shows how the data flows.\n",
    "```\n",
    "(0, 0) <-- zeroValue\n",
    "\n",
    "[1, 2]                  [3, 4]\n",
    "\n",
    "0 + 1 = 1               0 + 3 = 3\n",
    "0 + 1 = 1               0 + 1 = 1\n",
    "\n",
    "1 + 2 = 3               3 + 4 = 7\n",
    "1 + 1 = 2               1 + 1 = 2       \n",
    "    |                       |\n",
    "    v                       v\n",
    "  (3, 2)                  (7, 2)\n",
    "      \\                    / \n",
    "       \\                  /\n",
    "        \\                /\n",
    "         \\              /\n",
    "          \\            /\n",
    "           \\          / \n",
    "           ------------\n",
    "           |  combOp  |\n",
    "           ------------\n",
    "                |\n",
    "                v\n",
    "             (10, 4)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter & Sorting\n",
    "\n",
    "Filter can be used to remove or filter items from an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 2, 3, 1, 2, 3, 4, 5, 6, 7]\n",
      "[2, 2, 4, 6]\n"
     ]
    }
   ],
   "source": [
    "isEven = lambda x: x %2 == 0\n",
    "\n",
    "print(a.collect())\n",
    "print(a.filter(isEven).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['root', 'x', '0', '0', 'root', '/root', '/bin/bash']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passwd.map( lambda x : x.split(':' ) )\\\n",
    "   .filter( lambda x : x[0] == 'root' )\\\n",
    "   .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sortBy** and **sortByKey** serve a similar role as takeOrdered but sorts an RDD rather than the returned results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "passwdLines = open('/etc/passwd', 'r').readlines()\n",
    "passwd = sc.parallelize( passwdLines )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('root', '/bin/bash'),\n",
       " ('daemon', '/usr/sbin/nologin'),\n",
       " ('bin', '/usr/sbin/nologin')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userAndShell = passwd.map( lambda x: x.rstrip('\\n').split(':') )\\\n",
    "    .map( lambda y: ( y[0], y[6] ) )\n",
    "userAndShell.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sortBy( cmp: Func, ascending: Boolean)` takes a function that returns the sort key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('root', '/bin/bash'),\n",
       " ('daemon', '/usr/sbin/nologin'),\n",
       " ('bin', '/usr/sbin/nologin')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userAndShell.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_apt', '/usr/sbin/nologin'),\n",
       " ('backup', '/usr/sbin/nologin'),\n",
       " ('bin', '/usr/sbin/nologin')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userAndShell.sortBy(lambda x : x[0] ).take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('root', '/bin/bash'), ('jovyan', '/bin/bash'), ('sync', '/bin/sync')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userAndShell.sortBy(lambda x : x[1] ).take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sortByKey( ascending: Boolean)` assumes the data is in (k,v) pairs. In this case, the example is the same as sortByUser above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_apt', '/usr/sbin/nologin'),\n",
       " ('backup', '/usr/sbin/nologin'),\n",
       " ('bin', '/usr/sbin/nologin')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userAndShell.sortByKey().take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Operations\n",
    "\n",
    "**union** and **intersection** produce new RDD's where the elements can be thought of as being in a set. **distinct** returns the unique set of items in an RDD (*i.e.* converting a multi-set to a set). **sample**(withReplacement:Boolean, fraction:Float, [seed:int]) draws samples with or without replacement. Sample produces more representative samples with larger datasets and has seemingly erratic behavior with small sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 2, 3, 1, 2, 3, 4, 5, 6, 7]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 99, 22, -77]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 2, 3, 1, 2, 3, 4, 5, 6, 7, 2, 3, 99, 22, -77]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.union(b).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.intersection(b).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`subtract` removes items from the RDD that are contained in a second RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 2, 3, 1, 2, 3, 4, 5, 6, 7]  -  [2, 3, 99, 22, -77]  =  [4, 1, 5, 6, 7, 7]\n"
     ]
    }
   ],
   "source": [
    "print(a.collect(), \" - \", b.collect(), \" = \", a.subtract(b).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6, 7, 3, 1, 5]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A has  10 items\n",
      "The sample has  3 items:  [2, 7, 7]\n"
     ]
    }
   ],
   "source": [
    "print(\"A has \", a.count(), \"items\")\n",
    "s = a.sample(True, 0.2)\n",
    "print(\"The sample has \", s.count(), \"items: \", s.collect())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
